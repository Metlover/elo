% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calibration.R
\name{elo.calibration}
\alias{elo.calibration}
\alias{calibration}
\alias{calibration.elo.run}
\alias{calibration.elo.glm}
\alias{calibration.elo.running}
\alias{calibration.elo.markovchain}
\alias{calibration.elo.winpct}
\alias{calibration.elo.colley}
\title{Calculate calibration}
\usage{
calibration(object, subset, ...)

\method{calibration}{elo.run}(object, subset, ...)

\method{calibration}{elo.glm}(object, subset, ...)

\method{calibration}{elo.running}(object, subset, running = TRUE, ...)

\method{calibration}{elo.markovchain}(object, subset, ...)

\method{calibration}{elo.winpct}(object, subset, ...)

\method{calibration}{elo.colley}(object, subset, ...)
}
\arguments{
\item{object}{An object}

\item{subset}{(optional) A vector of indices on which to calculate calibration.}

\item{...}{Other arguments (not in use at this time).}

\item{running}{logical, denoting whether to use the running predicted values.}
}
\description{
Calculate the calibration (sum of times teams are predicted to win over the
number of times they actually won) for a given model. Perfect calibration
would be a score of 1, over-prediction would be a score greater than 1, and
under-prediction would be a score less than 1.
}
